# C-HMCNN

The baseline code and data for the paper "[Coherent Hierarchical Multi-label Classification Networks](https://proceedings.neurips.cc/paper/2020/hash/6dd4e10e3296fa63738371ec0d5df818-Abstract.html)".
Later expanded in order to deal with multiple explainations.

## Author

The original loss and consistency layer was implemented by [Eleonora Giunchiglia](https://www.cs.ox.ac.uk/people/eleonora.giunchiglia/) and [Thomas Lukasiewicz](https://www.cs.ox.ac.uk/people/thomas.lukasiewicz/).
Later expanded by Samuele Bortolotti.

## About

The current implementation is developed on top of the C-HMCNN architecture. 
And it offers the possibility to exploit both the approach introduced by the original authors and the one we propose.

## Getting Started

Follow these instructions to set up the project on your PC.

Moreover, to facilitate the use of the application, a Makefile has been provided; to see its functions, simply call the appropriate help command with [GNU/Make](https://www.gnu.org/software/make/)

 ```bash
 make help
 ```

### Clone the repository

 ```bash
 git clone https://github.com/samuelebortolotti/C-HMCNN.git
 cd C-HMNCNN
 ```

### Install the requirements

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

> **Note**: it might be convenient to create a virtual enviroment to handle the dependencies.
> 
> The `Makefile` provides a simple and convenient way to manage Python virtual environments (see [venv](https://docs.python.org/3/tutorial/venv.html)).
> In order to create the virtual enviroment and install the requirements be sure you have the Python 3.9 (it should work even with more recent versions, however I have tested it only with 3.9)
> ```bash
> make env
> source ./venv/chmncnn/bin/activate
> make install
> ```
> Remember to deactivate the virtual enviroment once you have finished dealing with the project
> ```bash
> deactivate
> ```

### Generate the code documentation

The automatic code documentation is provided [Sphinx v4.5.0](https://www.sphinx-doc.org/en/master/).

In order to have the code documentation available, you need to install the development requirements

```bash
pip install --upgrade pip
pip install -r requirements.dev.txt
```

Since Sphinx commands are quite verbose, I suggest you to employ the following commands using the `Makefile`.

```bash
make doc-layout
make doc
```

The generated documentation will be accessible by opening `docs/build/html/index.html` in your browser, or equivalently by running

```bash
make open-doc
```

However, for the sake of completeness one may want to run the full Sphinx commands listed here.

```bash
sphinx-quickstart docs --sep --no-batchfile --project c-chmnnn --author "Samuele Bortolotti, Eleonora Giunchiglia and Thomas Lukasiewicz"  -r 0.1  --language en --extensions sphinx.ext.autodoc --extensions sphinx.ext.napoleon --extensions sphinx.ext.viewcode --extensions myst_parser
sphinx-apidoc -P -o docs/source .
cd docs; make html
```

> **Note**: executing the second list of command will lead to a slightly different documentation with respect to the one generated by the `Makefile`.
> This is because the above listed commands do not customize the index file of Sphinx. This is because the above listed commands do not customize the index file of Sphinx.

## Giunchiglia and Lukasiewicz approach

Here there is the list of all the functionality provided by Giunchiglia et al approach.

### Experiment

To set up base C-HMCNN you can run:

```bash
  python -m chmncc experiment [EXP_NAME] [EPOCHS_NUMBER] \
  --dataset <dataset_name> --seed <seed_num> --device <device_num> \
  --batch-size <batch_size> --test-batch-size <test_batch_size> \
  --learning-rate <learning-rate> --weigh-decay <weigh-decay> \
  --day <dry> --project <project> --wandb <wandb> \
  --giunchiglia true \
```

Example:

```bash
python -m chmncc experiment exp_1 10 \
--dataset cellcycle_FUN --seed 0 --device 0
```

**Note:** the parameter passed to "dataset" must end with: '_FUN', '_GO', or '_others'.

If you want to execute the model for 10 seeds you can modify the script `main_script.sh` and execute it.

The results will be written in the folder `results/` in the file `<dataset_name>.csv`.

### Hyperparameters search

If you want to execute again the hyperparameters search you can modify the script `script.sh`according to your necessity and execute it. 


### Architecture

The code was run on a Titan Xp with 12GB memory. A description of the environment used and its dependencies is given in `c-hmcnn_enc.yml`.

By running the script `main_script.sh` we obtain the following results (average over the 10 runs):

| Dataset       | Result |
| ---           | ----   |
| Cellcycle_FUN | 0.255  |
| Derisi_FUN    | 0.195  |
| Eisen_FUN     | 0.306  |
| Expr_FUN      | 0.302  |
| Gasch1_FUN    | 0.286  |
| Gasch2_FUN    | 0.258  |
| Seq_FUN       | 0.292  |
| Spo_FUN       | 0.215  |
| Cellcycle_GO  | 0.413  |
| Derisi_GO     | 0.370  |
| Eisen_GO      | 0.455  |
| Expr_GO       | 0.447  |
| Gasch1_GO     | 0.436  |
| Gasch2_GO     | 0.414  |
| Seq_GO        | 0.446  |
| Spo_GO        | 0.382  |
| Diatoms_others| 0.758  |
| Enron_others  | 0.756  |
| Imclef07a_others | 0.956 |
| Imclef07d_others | 0.927 |


### Reference
```
@inproceedings{giunchiglia2020neurips,
    title     = {Coherent Hierarchical Multi-label Classification Networks},
    author    = {Eleonora Giunchiglia and
               Thomas Lukasiewicz},
    booktitle = {34th Conference on Neural Information Processing Systems (NeurIPS 2020)},
    address = {Vancouver, Canada},
    month = {December},
    year = {2020}
}
```

## Our approach

For now, the approach consists in training a simple CNN, namely [LeNet5](https://en.wikipedia.org/wiki/LeNet), on CIFAR100 with layer proposed by E. Giunchiglia and T. Lukasiewicz.

### Dataset

In order to perform an experiment, you have to prepare the Hierarchical [CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) first.

Therefore, in order to download and filter the dataset you can employ the following `Makefile` command:

```bash
make dataset
```

Or equivalently, the following python command

```bash
python -m chmncc dataset
```

### Train and test

After the dataset is ready, you can perform an experiment by typing:

```bash
make experiment
```

Or equivalently, the following python command

```bash
python -m chmncc experiment [EXP_NAME] [EPOCHS_NUMBER] \
--learning-rate <learning-rate> --batch-size <train-batch-size> \
--test-batch-size <test-batch-size> --device <device>
```

As an effective representation, a valid command is the following:

```bash
python -m chmncc experiment "chmncc" 20 --learning-rate 0.001 \
--batch-size 10 --test-batch-size 10 --device cuda
```
